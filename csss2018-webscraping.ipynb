{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascraping : Web Scraping / APIs\n",
    "Dec 10th, 2018 - Javier Garcia-Bernardo, Anna Keuchenius & Allie Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Requirements\n",
    "import requests               # Simple HTTP operations (GET and POST)\n",
    "import selenium               # Loads dynamic (javascript) pages\n",
    "import json                   # Parsing the responses from APIs\n",
    "import re                     # Python library for parsing regular expressions\n",
    "from bs4 import BeautifulSoup # Parsing HTML\n",
    "import pandas as pd           # Read tables\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is datascraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data scraping](https://en.wikipedia.org/wiki/Web_scraping) is a method for extracting data from the web. There are many techniques which can be used for web scraping — ranging from requiring human involvement (“human copy-paste”) to fully automated systems (using computer vision). Somewhere in the middle is HTML parsing, which we will describe here.\n",
    "\n",
    "Web scraping using [HTML parsing](https://en.wikipedia.org/wiki/Web_scraping#HTML_parsing) is often used on webpages which share similar HTML structure. For example, you might want to scrape the ingredients from chocolate chip cookie recipes to identify correlations between ingredients and five-star worthy cookies, or you might want to predict who will win March Madness by looking at game play-by-plays, or you want to know all the local pets up for adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Static Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta charset=\"utf-8\" />\n",
      "<link rel=\"shortcut icon\" href=\"https://www.boulderhumane.org/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 7 (http://drupal.org)\" />\n",
      "<meta name=\"viewport\" content=\"width=1000px, initial-scale=1.0, maximum-scale=1.0\" />\n",
      "<title>Dogs Available for Adoption | Humane Society of Boulder Valley</title>\n",
      "<link type=\"text/css\" rel=\"stylesheet\n"
     ]
    }
   ],
   "source": [
    "urls = [\"https://www.boulderhumane.org/animals/adoption/dogs\", \n",
    "         \"https://www.boulderhumane.org/animals/adoption/cats\", \n",
    "         \"https://www.boulderhumane.org/animals/adoption/adopt_other\"]\n",
    "\n",
    "page = requests.get(urls[0])\n",
    "# Extractt \n",
    "html = page.text\n",
    "print(html[:500]) # Print the first 500 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information: parsing html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a webpage, your web browser renders an HTML document with CSS and Javascript to produce a visually appealing page. (See the HTML above.) [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library for parsing HTML. We'll use it to extract all of the names, ages, and breeds of the [dogs](https://www.boulderhumane.org/animals/adoption/dogs), [cats](https://www.boulderhumane.org/animals/adoption/cats), and [small animals](https://www.boulderhumane.org/animals/adoption/adopt_other) currently up for adoption at the Boulder Humane Society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the feature of these pages which we are exploiting is their repeated HTML structure. Every animal listed has the following HTML variant:\n",
    "```{html}\n",
    "<div class=\"views-row ... \">\n",
    "  ...\n",
    "  <div class=\"views-field views-field-field-pp-animalname\">\n",
    "    <div class=\"field-content\">\n",
    "      <a href=\"/animals/adoption/\" title=\"Adopt Me!\">Romeo</a>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-primarybreed\">\n",
    "    <div class=\"field-content\">New Zealand</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-secondarybreed\">\n",
    "    <div class=\"field-content\">Rabbit</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-age\">\n",
    "    ...\n",
    "    <span class=\"field-content\">0 years 2 months</span>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-gender\">\n",
    "    ...\n",
    "    <span class=\"field-content\">Male</span>\n",
    "  </div>\n",
    "  ...\n",
    "</div>\n",
    "``` \n",
    "So to get at the HTML object for each pet, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = soup.find_all('div', {'class': re.compile('.*views-row.*')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, find all of the `div` tags with the `class` attribute which contains the substring `views-row`. \n",
    "\n",
    "**notice that we use regex here in the re.compile statement. Regex helps to find patterns in text. More info on regex [here](https://docs.python.org/3/library/re.html). The wildcard .* matches everything (all characters).\n",
    "\n",
    "Next to grab the name, breeds, and ages of these pets, we’ll grab the children of each pet HTML object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Blue', u'Coonhound, Treeing Walker', u'', u'Age:7 years 5 months')\n",
      "(u'Calvin', u'Pointer, German Shorthaired', u'Mix', u'Age:1 year 9 months')\n",
      "(u'Mona', u'Greyhound', u'Retriever, Labrador', u'Age:8 years 1 month')\n",
      "(u'Iris', u'Siberian Husky', u'Saint Bernard', u'Age:7 years 0 months')\n",
      "(u'Lucia', u'Chihuahua, Short Coat', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Karma', u'Terrier, American Pit Bull', u'Mix', u'Age:3 years 0 months')\n",
      "(u'Chuco', u'Terrier, American Pit Bull', u'Mix', u'Age:3 years 7 months')\n",
      "(u'Harper', u'Terrier, American Pit Bull', u'Mix', u'Age:1 year 1 month')\n",
      "(u'Lupita', u'Bulldog, American', u'Mix', u'Age:2 years 0 months')\n",
      "(u'Winston', u'Chihuahua, Short Coat', u'Mix', u'Age:1 year 0 months')\n",
      "(u'Beans', u'Terrier, American Pit Bull', u'Mix', u'Age:0 years 2 months')\n",
      "(u'Coco', u'Bulldog, English', u'Mix', u'Age:0 years 5 months')\n",
      "(u'Oliver', u'Alaskan Klee Kai', u'', u'Age:7 years 10 months')\n",
      "(u'Canon', u'Siberian Husky', u'Mix', u'Age:3 years 0 months')\n",
      "(u'Eve', u'Miniature Pinscher', u'', u'Age:0 years 4 months')\n",
      "(u'Buck', u'Retriever, Labrador', u'Mix', u'Age:1 year 2 months')\n",
      "(u'Idgie', u'Siberian Husky', u'Mix', u'Age:0 years 3 months')\n",
      "(u'Ruth', u'Siberian Husky', u'Mix', u'Age:0 years 3 months')\n"
     ]
    }
   ],
   "source": [
    "head = \"views-field views-field-field-pp-\"\n",
    "for pet in pets:\n",
    "    name = pet.find('div', {'class': head + 'animalname'}).get_text(strip=True)\n",
    "    primary_breed = pet.find('div', {'class': head + 'primarybreed'}).get_text(strip=True)\n",
    "    secondary_breed = pet.find('div', {'class': head + 'secondarybreed'}).get_text(strip=True)\n",
    "    age = pet.find('div', {'class': head + 'age'}).get_text(strip=True)\n",
    "    print(name, primary_breed, secondary_breed, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each call to `find` is getting the children of a pet object, in particular, the `div`s with `class` attributes which look like `views-field views-field-field-pp-*`. Feel free to replace the above code with the cat or small animal pages provided and see how the output changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often use BeautifulSoup's .find or .find_all methods. However, there are also other useful methods that one can use.  For example, .findNext of .nextSibling. More in the [docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (especially under tab 'method names'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Ib: Extract Tables from Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly read a table from html and converge it to a Pandas dataframe with pandas method read_html. Then we can use the regular pandas methods to manipulate or filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Image</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Often eaten with ketchup or brown sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacon, egg and cheese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Breakfast sandwich, usually with fried or scra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagel toast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Pressed, toasted bagel filled with vegetables ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baked bean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Canned baked beans on white or brown bread, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bánh mì[4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Filling is typically meat, but can contain a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbecue[5][6][7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Served on a bun, with chopped, sliced, or shre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Barros Jarpa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Ham and cheese, usually mantecoso, which is si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Barros Luco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Beef (usually thin-cut steak) and cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bauru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Melted cheese, roast beef, tomato, and pickled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beef on weck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States(Buffalo, New York)</td>\n",
       "      <td>Roast beef on a Kummelweck roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beirute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Melted cheese, sliced fresh tomatoes with oreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BLT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Named for its ingredients: bacon, lettuce, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bocadillo de calamares</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Baguette bread filled with fried squid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bologna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States, Canada</td>\n",
       "      <td>Pre-sliced and sometimes fried bologna sausage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boloney (Bologna) salad sandwich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE Pennsylvania</td>\n",
       "      <td>A mixture of bologna sausage and sweet gherkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bosna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Usually grilled on white bread, containing a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bratwurst Sandwich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "      <td>The Bratwurst sandwich is a popular street foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Breakfast roll</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom and Ireland</td>\n",
       "      <td>Convenience dish on a variety of bread rolls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Breakfast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Typically a scrambled or fried egg, cheese, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>British Rail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Reference to the poor quality of catering on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Name Image                            Origin  \\\n",
       "0                              Bacon   NaN                    United Kingdom   \n",
       "1              Bacon, egg and cheese   NaN                     United States   \n",
       "2                        Bagel toast   NaN                            Israel   \n",
       "3                         Baked bean   NaN                     United States   \n",
       "4                         Bánh mì[4]   NaN                           Vietnam   \n",
       "5                  Barbecue[5][6][7]   NaN                     United States   \n",
       "6                       Barros Jarpa   NaN                             Chile   \n",
       "7                        Barros Luco   NaN                             Chile   \n",
       "8                              Bauru   NaN                            Brazil   \n",
       "9                       Beef on weck   NaN  United States(Buffalo, New York)   \n",
       "10                           Beirute   NaN                            Brazil   \n",
       "11                               BLT   NaN                     United States   \n",
       "12            Bocadillo de calamares   NaN                             Spain   \n",
       "13                           Bologna   NaN             United States, Canada   \n",
       "14  Boloney (Bologna) salad sandwich   NaN                   NE Pennsylvania   \n",
       "15                             Bosna   NaN                           Austria   \n",
       "16                Bratwurst Sandwich   NaN                           Germany   \n",
       "17                    Breakfast roll   NaN        United Kingdom and Ireland   \n",
       "18                         Breakfast   NaN                     United States   \n",
       "19                      British Rail   NaN                    United Kingdom   \n",
       "\n",
       "                                          Description  \n",
       "0             Often eaten with ketchup or brown sauce  \n",
       "1   Breakfast sandwich, usually with fried or scra...  \n",
       "2   Pressed, toasted bagel filled with vegetables ...  \n",
       "3   Canned baked beans on white or brown bread, so...  \n",
       "4   Filling is typically meat, but can contain a w...  \n",
       "5   Served on a bun, with chopped, sliced, or shre...  \n",
       "6   Ham and cheese, usually mantecoso, which is si...  \n",
       "7            Beef (usually thin-cut steak) and cheese  \n",
       "8   Melted cheese, roast beef, tomato, and pickled...  \n",
       "9                     Roast beef on a Kummelweck roll  \n",
       "10  Melted cheese, sliced fresh tomatoes with oreg...  \n",
       "11  Named for its ingredients: bacon, lettuce, and...  \n",
       "12             Baguette bread filled with fried squid  \n",
       "13  Pre-sliced and sometimes fried bologna sausage...  \n",
       "14  A mixture of bologna sausage and sweet gherkin...  \n",
       "15  Usually grilled on white bread, containing a b...  \n",
       "16  The Bratwurst sandwich is a popular street foo...  \n",
       "17  Convenience dish on a variety of bread rolls, ...  \n",
       "18  Typically a scrambled or fried egg, cheese, an...  \n",
       "19  Reference to the poor quality of catering on t...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_sandwiches\",header=0)[0]\n",
    "\n",
    "# Write table to CSV\n",
    "#table.to_csv(\"filenamehere.csv\")\n",
    "\n",
    "# Output the top rows of the table\n",
    "table.head((20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Image</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Often eaten with ketchup or brown sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>British Rail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Reference to the poor quality of catering on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cheese and pickle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Slices of cheese (typically Cheddar) and pickl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Chip butty[11][12][13][14]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sliced white bread (or a large, flat bread rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Corned beef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Corned beef often served with a condiment such...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name Image          Origin  \\\n",
       "0                        Bacon   NaN  United Kingdom   \n",
       "19                British Rail   NaN  United Kingdom   \n",
       "29           Cheese and pickle   NaN  United Kingdom   \n",
       "37  Chip butty[11][12][13][14]   NaN  United Kingdom   \n",
       "44                 Corned beef   NaN  United Kingdom   \n",
       "\n",
       "                                          Description  \n",
       "0             Often eaten with ketchup or brown sauce  \n",
       "19  Reference to the poor quality of catering on t...  \n",
       "29  Slices of cheese (typically Cheddar) and pickl...  \n",
       "37  Sliced white bread (or a large, flat bread rol...  \n",
       "44  Corned beef often served with a condiment such...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals_from_uk = table[table['Origin'] == 'United Kingdom']\n",
    "animals_from_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part  II: Dynamic Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we requested webpages that required no [Javascript](https://en.wikipedia.org/wiki/JavaScript). In other words, there was no input required on the users' end to view the content of the page (e.g. a login). Let's try a more complicated example of webscraping where content is loaded dynamically.\n",
    "\n",
    "[Selenium](https://www.seleniumhq.org/download/)\n",
    "\n",
    "Some advantages of HTML scraping with Selenium it: can handle javascript, get **HTML** back after the Javascript has been rendered, can behave like a person. The disadvantage of using Selenium is that it is generally slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements (one of the below):\n",
    "- Firefox + geckodriver (https://github.com/mozilla/geckodriver/releases)\n",
    "- Chrome + chromedriver (https://sites.google.com/a/chromium.org/chromedriver/)\n",
    "    \n",
    "Note: geckodriver/chromedriver must have execution permissions (chmod +x geckodriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the browser and define how much are you willing to wait for a page to load. (Many times this is not needed but it doesn't hurt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the driver (change the executable path to geckodriver_mac.exe or geckodriver.exe)\n",
    "driver = selenium.webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "#driver = selenium.webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [xkcd](https://xkcd.com) and click through the comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the xkcd website\n",
    "driver.get(\"https://xkcd.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the 'random' buttom and go to a random comic\n",
    "random_element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a')\n",
    "random_element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the 'next' button and go to the next comic\n",
    "next_element = driver.find_element_by_css_selector(\"a[rel='next']\")\n",
    "next_element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the examples above, there are several ways to identify elements on the page, such as with the xpath or by css selectors. You can find more methods in the [docs](https://onlinetraining.etestinghub.com/webdriver-methods-web-elements/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an attribute of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Earth clearly hasn't been inspected, since it's definitely contaminated with salmonella.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visit a webpage which requires a login. Signing in to Facebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DO NOT WRITE YOUR PASSWORD IN NOTEBOOKS!!\n",
    "fb_email = \"your-email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to Facebook\n",
    "driver = selenium.webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "driver.get(\"https://www.facebook.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You now have 20 seconds to fill out your password\n"
     ]
    }
   ],
   "source": [
    "# Send email and password\n",
    "driver.find_element_by_xpath('//*[@id=\"email\"]').send_keys(fb_email)\n",
    "time.sleep(20)\n",
    "print(\"You now have 20 seconds to fill out your password\")\n",
    "# driver.find_element_by_xpath('//*[@id=\"pass\"]').send_keys(fb_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on login\n",
    "driver.find_element_by_xpath('//*[@id=\"loginbutton\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to CSS Amsterdam Website\n",
    "driver.get(\"https://www.facebook.com/CSSamsterdam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vind_ik_leuks = driver.find_element_by_xpath(\"//*[contains(text(), 'Vind ik leuk')]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to close your browser!\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow users to access large amounts of data, companies may provide an [Application Programming Interface (API)](https://en.wikipedia.org/wiki/Application_programming_interface). Often these request are handled via PUT and POST HTTP requests. For example, to make a request from the Twitter API:\n",
    "\n",
    "```{bash}\n",
    "curl --request GET \n",
    " --url 'https://api.twitter.com/1.1/search/tweets.json?q=nasa&result_type=popular' \n",
    " --header 'authorization: OAuth oauth_consumer_key=\"consumer-key-for-app\", ... , \n",
    " oauth_token=\"access-token-for-authed-user\", oauth_version=\"1.0\"'\n",
    " ```\n",
    "\n",
    "APIs often return data in the format of [Javascript Object Notation (JSON)](https://en.wikipedia.org/wiki/JSON). For example:\n",
    "\n",
    "```{json}\n",
    "{\"status\": 200, \"message\": \"hello world\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit APIs\n",
    "\n",
    "Next, let's try a more typical example of an API. The perks of this approach: \n",
    "- (a) send a request and get back JSON, \n",
    "- (b) chances are that somebody else has created a Python wrapper for you, but keep in mind that \n",
    "- (c) APIs have limits.\n",
    "\n",
    "Let's consider a common API example -- Twitter. To get started:\n",
    "- Get a key: https://apps.twitter.com/\n",
    "- Documentation: https://dev.twitter.com/rest/public\n",
    "- Find a library: https://dev.twitter.com/resources/twitter-libraries (We'll use https://github.com/tweepy/tweepy)\n",
    "\n",
    "Limitations: 100 messages / query, 180 messages every 15 min, & only the last seven days of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the twitter keys that were created for this demo. Please don't use these keys after this datascraping course any longer. GENEREALLY, NEVER STORE PASSWORD, KEYS OR OTHER SENSITIVE INFORMATION IN NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_keys = {}\n",
    "d_keys['CONSUMER_KEY'] = 'iISHnFwAbqlBehAP6RxUIjawG'\n",
    "d_keys['CONSUMER_SECRET'] = 'XJTe1LlhZMSaRHE7SH1paZGwzb2HNbd5GyeDgI9HoLVoeBoBVM'\n",
    "d_keys['ACCESS_KEY'] = '3001441779-A3BEX2aY86j7yOsjwiZ7bRYmFkHnfpsSaPVtaBs'\n",
    "d_keys['ACCESS_SECRET'] = 'qa1ZKpMZPmKqoKJUvD4Gdw3GWhuMRHtIrFvkR7DYoaLHc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "It's working... https://t.co/HfqQ8QDO1N\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "RT @ewhitmore: Did you know @sfiscience in #SantaFe is one of the top places in the world to explore #ComplexSystems &amp; Complexity Science?…\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "RT @DavidFeldman: This is a great program!  Highly recommended.  I was a student in 1996 and have served as the director since 2017.  The s…\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "RT @StefaniCrabtree: Our amazing collaborator on the #archaeoEcology project run through @sfiscience, Andy Dugmore talking about learning f…\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "A fun, very quick intro to the field of #AI c/o our own @MelMitchell1 + @LetsWorkHappy #podcast...come for the robo… https://t.co/JPSJChD6IT\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "Transcend disciplinary boundaries, take intellectual risks, and ask big questions about complex systems!  \n",
      "\n",
      "Deadlin… https://t.co/lMJyXj8r81\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "Love wondering about the ancient arms races that drove the #complexity of ancient oceans?\n",
      "\n",
      "Now you can make a PhD o… https://t.co/XleETqcUxv\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "2/2 \n",
      "...so we get a hint at why \"the uncanny efficacy of mathematics\" AND a potential revelatory link between… https://t.co/kvteGEuSxh\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "1/2 New work in algorithmic information theory seems intimately related to Adi Livnat of @HaifaUniversity's talk at… https://t.co/5Zm0coVaR8\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "@seanmcarroll Improvising to this piece - that's the musical equivalent of a PhD in #complexity https://t.co/HS37pLJpJr\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "@platobooktour @hiddenforcespod We keep this around in your honor... https://t.co/Tl5LXDbUfr\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "A very warm look from within at the growing worldwide network of complexity science institutions and research - tha… https://t.co/wMreLfVwuG\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "Science undergrads! Join us in summer 2019 for 10 weeks of innovative research &amp; attentive mentorship. Discard disc… https://t.co/BmnIikDovC\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "Extremely useful flowchart for navigating a new scene (university departments, dolphin pods, bar fights, etc.):\n",
      "\n",
      "c/… https://t.co/qeSFpBIq4x\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "Speaking of many-model thinking, where would #science be without #philosophy? \n",
      "\n",
      "Here's our 2011 Miller Scholar, the… https://t.co/0y1vZl0z1G\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "\"Contemplation can occur anywhere — in the shower, on a commute, while hiking (the SFI in New Mexico, set among hil… https://t.co/DFkMrEXAf9\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "RT @ricard_sole: Last night at @sfiscience. Great work, lots of new questions, exciting research on NK tech landscapes with @svalver , terr…\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "@neurovium @svalver @george_bassel @turinginst Luckily, we DO have hundreds of other public talks on YouTube and wi… https://t.co/3lPS4oJThI\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "\"Versatility\" in social networks is connection to distantly-related nodes: people who don't think like you, don't l… https://t.co/9qubDZKlNq\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "@svalver @neurovium @george_bassel @turinginst Sadly no, it wasn't streamed or recorded.\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "RT @svalver: Looking forward to the SFI seminar \"Information processing and distributed computation in plant organs\" by @george_bassel.  ht…\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "This is the easiest way you can help spread complex systems literacy, in an age that definitely needs it:\n",
      "\n",
      "#MOOC… https://t.co/Fx3deE4cQR\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "@kedwardbear @george_bassel Our apologies! There was an editorial snafu with our events...thanks for catching that… https://t.co/BD9fcGHm1d\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "“We have any number of ethical issues right now, with the #AI systems we have...it’s unfortunate to distract from t… https://t.co/OVN4wh7vrw\n",
      "__________\n",
      "(u'sfiscience', None, 28802)\n",
      "How to stifle #innovation, c/o @Bloomberg:\n",
      "\n",
      "[Fmr SFI Pres] 'Geoffrey West showed...as firms get older, funding for… https://t.co/NvrBGelt2n\n",
      "Number of results: 25 (25 new)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def twitter(d_keys,query, num_results=1000):\n",
    "    # Authtentify\n",
    "    auth = tweepy.OAuthHandler(d_keys[\"CONSUMER_KEY\"], d_keys[\"CONSUMER_SECRET\"])\n",
    "    auth.set_access_token(d_keys[\"ACCESS_KEY\"], d_keys[\"ACCESS_SECRET\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # We want 1000 tweets\n",
    "    result_count = 0\n",
    "    last_id = None\n",
    "    \n",
    "    # Max 180 tweets 15 min\n",
    "    cumulative = 0\n",
    "\n",
    "    #While we don't have them\n",
    "    while (result_count <  num_results):\n",
    "        previous_tweets = result_count\n",
    "        # Ask for more tweets, starting in the 'last_id' (identifier of the tweet)\n",
    "        results = api.search(q = query,\n",
    "                              count = 90, max_id = last_id, result_type=\"recent\")\n",
    "                                # geocode = \"{},{},{}km\".format(latitude, longitude, max_range) #for geocode\n",
    "\n",
    "        # For each tweet extract some info (JSON structure)\n",
    "        for result in results:\n",
    "            result_count += 1\n",
    "            user = result.user.screen_name\n",
    "            text = result.text\n",
    "            followers_count = result.user.followers_count\n",
    "            time_zone = result.user.time_zone\n",
    "            print(\"_\"*10)\n",
    "            print(user,time_zone,followers_count)\n",
    "            print(text)\n",
    "\n",
    "        # Keep the last_id to know where to continue\n",
    "        last_id = int(result.id)-1\n",
    "        new_tweets = result_count - previous_tweets\n",
    "\n",
    "        print (\"Number of results: {} ({} new)\".format(result_count,new_tweets))\n",
    "\n",
    "        # If we don't get new tweets exit\n",
    "        if new_tweets == 0: \n",
    "            break\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        if ((result_count + 90) // 150) > cumulative:\n",
    "            cumulative += 1\n",
    "            time.sleep(15*60)\n",
    "\n",
    "\n",
    "twitter(d_keys,\"from:sfiscience\", num_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Scraping Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hidden\" APIs\n",
    "\n",
    "First, let's try and access what we are calling a \"hidden\" API. That is, we investigate the resources requested by a webpage (e.g. a list of faculty), and make requests directly to that API. \n",
    "\n",
    "We will do this for the website: https://www.uvm.edu/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, vist uvm.edu/directory and open the network tab as you do a search in this directory.  \n",
    "Copy the get url, and paste it on the website https://curl.trillworks.com/, that will convert directly to a python requests command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_names(letters):\n",
    "    params = (\n",
    "        ('name', letters),\n",
    "        ('request_num', '1'),\n",
    "    )\n",
    "\n",
    "    response = requests.get('https://www.uvm.edu/directory/api/query_results.php', params=params)\n",
    "    if response.ok == True:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_names(\"john smith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Affiliate', u'jfsmith@uvm.edu', u'John F. Smith')\n",
      "(u'Student', u'dsmith41@uvm.edu', u'David John Smith')\n",
      "(u'Affiliate', u'jfsmith@uvm.edu', u'John F. Smith')\n",
      "(u'Student', u'dsmith41@uvm.edu', u'David John Smith')\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "for i, person in enumerate(response_json[\"data\"]):\n",
    "#     display(person)\n",
    "    if i == 10: \n",
    "        break # Make sure we don't print too much\n",
    "        \n",
    "    print(person[\"edupersonprimaryaffiliation\"][\"0\"], person[\"edupersonprincipalname\"][\"0\"], person[\"cn\"][\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Session ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites changes their html all the time. Therefore it is worthwhile to make your scraper robust.  There are a few tips we have on how to do this, and you might know some other tricks too.\n",
    "\n",
    "- Don't make your scraper language dependent. Your browser setting will influence the text displayed on websites. So if you extracting elements by text, this is sensitive your browser setting, and to the general language of the website. It's better not extract elements by text.\n",
    "- Save raw html. As we learned last week from Damian Trillings Database Management workshop, it is very good practice to save the entire html of the website in stead of only the elements you are interested in. That way, if the website has changed their html and your scraper brakes or is not extracing the right elements anymore, you can simply re-extract the information later from the raw html that you saved in your database.\n",
    "- Use drilldown method. Don't look for a class or attribute in the entire html, but first drill down the specific part of page. Very bad practice is to look for all elements of very general html attribute (such as 'row'), which returns a list, and then select the right index of that list.  This is very sensitive to html changes!\n",
    "- I always try to avoid xpath, for the same reason as above.\n",
    "- Track your progress. Build you scraper in a way so that it is not a problem if it crashes. Scrapers will always crash, almost by default. For example, your vpn connenction can shut down (if applicable), your internet connection might brake, the site your scraping might go down for a moment etc. Track your progress somewhere so that you can always turn your scraper on so that it start from where it left of.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawlers -- scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, scraping can be easily parallelized. Especially if you have several urls that need to be scraped independently. In case you do a search on website, and get many result pages, you can also parallelize your code; you can divide the work of scraping over several scrapers that all scrape several pages. However, then you need to put in place a way of tracking what has been scraped and what has not. Maybe some of you, us, have advice on how to do this? I personally use a tracking table in my database, that tracks the progress. \n",
    "\n",
    "Again, robustness is important: build your scrapers or crawlers in such a way that it is absolutely fine if a scraper dies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to parallelize scrapers, i.e. setup crawlers. One of the ways is to do this yourself, without an external service, by means of subprocess. Here is some simple code I wrote to do this. This spin_up_scrapers code spins up several scrapers, and check every x seconds if each scraper is still active. If one dies, another scraper is spin up.\n",
    "main.py is the scraper code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "nr_scrapers = 10\n",
    "nr_hours_scraping = 10\n",
    "\n",
    "def spin_up_scraper(nr_scrapers, nr_hours_scraping)\n",
    "    scraper_processes = []\n",
    "    for scraper_i in range(nr_scrapers):\n",
    "        p = subprocess.Popen(['python main.py'], shell=True,\n",
    "                                stdin=None, stdout=None, stderr=None, close_fds=True)\n",
    "\n",
    "        # Wait a few moments before starting the next scraper\n",
    "        time.sleep(20)\n",
    "        print(\"---------------Starting next scraper-------------------------------\")\n",
    "        scraper_processes.append(p)\n",
    "\n",
    "\n",
    "    # Check every minute if all scrapers are up, if one is down, start a new one\n",
    "    for minutes in range(60*nr_hours_scraping):\n",
    "        # Sleep 60 seconds till the next check\n",
    "        time.sleep(60)\n",
    "        for scraper in scraper_processes:\n",
    "            down = scraper.poll()\n",
    "            if down is None:\n",
    "                scraper_processes.remove(scraper)\n",
    "                print('----One scraper down. Starting a new one ------------------------')\n",
    "                p = subprocess.Popen(['python main.py'], shell=True,\n",
    "                                     stdin=None, stdout=None, stderr=None, close_fds=True)\n",
    "                scraper_processes.append(p)\n",
    "                time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrolling down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
